{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Temporal Encoding for Sequence Data\n",
    "class TemporalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=500):\n",
    "        super(TemporalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.encoding[:, :seq_len, :].to(x.device)\n",
    "\n",
    "# Modality-Specific Tokenizers\n",
    "class ModalityTokenizer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(ModalityTokenizer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# Shared Transformer Encoder\n",
    "class SharedTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers):\n",
    "        super(SharedTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=num_heads, dim_feedforward=2048, activation=\"gelu\"\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Task-Specific Heads\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, output_dim)  # Output: 6 (pose prediction: x, y, z, α, β, γ)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x.mean(dim=1))\n",
    "\n",
    "\n",
    "# Multi-Modal Transformer Model\n",
    "class MultiModalTransformer(nn.Module):\n",
    "    def __init__(self, modalities, embed_dim, num_heads, num_layers, max_seq_len=500):\n",
    "        super(MultiModalTransformer, self).__init__()\n",
    "        self.tokenizers = nn.ModuleDict({\n",
    "            name: ModalityTokenizer(input_dim, embed_dim) for name, input_dim in modalities.items()\n",
    "        })\n",
    "        self.temporal_encoding = TemporalEncoding(embed_dim, max_len=max_seq_len)\n",
    "        self.shared_transformer = SharedTransformer(embed_dim, num_heads, num_layers)\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            \"strawberry_picking\": TaskHead(embed_dim, output_dim=6),\n",
    "            \"vertebrae_scanning\": TaskHead(embed_dim, output_dim=6),\n",
    "            \"autonomous_vehicle\": TaskHead(embed_dim, output_dim=6),\n",
    "        })\n",
    "\n",
    "    def forward(self, inputs, task):\n",
    "        # Tokenize each modality and stack along modality dimension\n",
    "        tokenized_inputs = [self.tokenizers[name](inputs[name]) for name in inputs.keys()]\n",
    "        fused_tokens = torch.stack(tokenized_inputs, dim=1)  # (batch_size, num_modalities, embed_dim)\n",
    "        # Combine modalities by averaging embeddings\n",
    "        fused_tokens = fused_tokens.mean(dim=1)  # (batch_size, embed_dim)\n",
    "        # Add temporal encoding\n",
    "        encoded_tokens = self.temporal_encoding(fused_tokens.unsqueeze(1))  # Add sequence dimension\n",
    "        # Pass through shared transformer\n",
    "        shared_features = self.shared_transformer(encoded_tokens)\n",
    "        # Task-specific head\n",
    "        return self.task_heads[task](shared_features)\n",
    "\n",
    "# Instantiate the Model Modalities Dictionary: Include all required modalities\n",
    "modalities = {\n",
    "    \"vision\": 1024,          # Strawberry Picking\n",
    "    \"proprioception\": 256,\n",
    "    \"tactile\": 128,\n",
    "    \n",
    "    \"pose\": 64,              # Vertebrae Scanning\n",
    "    \"ultrasonic\": 128,\n",
    "    \"detected_position\": 64,  \n",
    "    \n",
    "    \"gps\": 64,               # Autonomous Vehicle\n",
    "    \"imu\": 256,\n",
    "    \"mmwave\": 512,\n",
    "    \"lidar\": 1024,\n",
    "    \"camera\": 1024,\n",
    "}\n",
    "\n",
    "# Instantiate the Model\n",
    "model = MultiModalTransformer(modalities, embed_dim=768, num_heads=12, num_layers=6)\n",
    "\n",
    "# Example Inputs\n",
    "inputs_strawberry = {\n",
    "    \"vision\": torch.rand(32, 1024),\n",
    "    \"proprioception\": torch.rand(32, 256),\n",
    "    \"tactile\": torch.rand(32, 128),\n",
    "}\n",
    "\n",
    "inputs_vertebrae = {\n",
    "    \"pose\": torch.rand(32, 64),\n",
    "    \"ultrasonic\": torch.rand(32, 128),\n",
    "    \"detected_position\": torch.rand(32, 64),  # Corrected key\n",
    "}\n",
    "\n",
    "inputs_vehicle = {\n",
    "    \"gps\": torch.rand(32, 64),\n",
    "    \"imu\": torch.rand(32, 256),\n",
    "    \"mmwave\": torch.rand(32, 512),\n",
    "    \"lidar\": torch.rand(32, 1024),\n",
    "    \"camera\": torch.rand(32, 1024),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with actual input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision input (strawberry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# resnet is a pretrained CNN with skip connections to prevent gradient vanishing\n",
    "# images are resized to 224x224 to match resnet50 input requirements\n",
    "# output: feature vector per image of size 2048\n",
    "# final fully connected layer projects 2048 to a lower 1024 embedding (original dimension of vision input)\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "images = torch.rand(32, 3, 224, 224)  # Example input images\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = nn.Linear(2048, 1024)  # Output dimension matches \"vision\" input_dim\n",
    "\n",
    "# vision_embeddings = resnet(images).detach()  # (32, 1024)\n",
    "processed_vision = resnet(images)  # Shape: (32, 1024)\n",
    "\n",
    "\n",
    "# inputs_strawberry[\"vision\"] = vision_embeddings\n",
    "inputs_strawberry[\"vision\"] = processed_vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidar input (Driving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# lidar generates 3D point cloud : 2048 points x 3 (x,y,z)\n",
    "# averaged along the point dimension to produce 1 vector to summarize 3D point cloud for the batch\n",
    "# linear layer maps mean from 3D to 1024-dim embedding \n",
    "\n",
    "# Simulate Lidar Embedding Extraction\n",
    "lidar_data = torch.rand(32, 2048, 3)  # Example point cloud (batch_size, num_points, dimensions)\n",
    "\n",
    "# lidar_embeddings = torch.mean(lidar_data, dim=1)  # Reduce to 1024-dim features\n",
    "lidar_embeddings = nn.Linear(3, 1024)(lidar_data.mean(dim=1))  # Project to 1024 dimensions\n",
    "\n",
    "inputs_vehicle[\"lidar\"] = lidar_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS input (Driving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# linear layer maps latitude,longitude to 64-dim embedding \n",
    "# this expands 2D to higher-dim, thus more compatible with transformer\n",
    "\n",
    "gps_data = torch.rand(32, 2)  # Example GPS coordinates (latitude, longitude)\n",
    "gps_embeddings = nn.Linear(2, 64)(gps_data)  # Project to 64 dimensions\n",
    "inputs_vehicle[\"gps\"] = gps_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Forward Pass for Strawberry Picking\n",
    "output_strawberry = model(inputs_strawberry, task=\"strawberry_picking\")\n",
    "print(output_strawberry.shape)  # Expected: (32, 6)\n",
    "\n",
    "# Forward Pass for Vertebrae Scanning\n",
    "output_vertebrae = model(inputs_vertebrae, task=\"vertebrae_scanning\")\n",
    "print(output_vertebrae.shape)  # Expected: (32, 6)\n",
    "\n",
    "# Forward Pass for Autonomous Vehicle\n",
    "output_vehicle = model(inputs_vehicle, task=\"autonomous_vehicle\")\n",
    "print(output_vehicle.shape)  # Expected: (32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(output_strawberry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "target = torch.rand(32, 6)  # Example target\n",
    "loss = criterion(output_strawberry, target)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
